{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extracted-information\" data-toc-modified-id=\"Extracted-information-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Extracted information</a></span></li><li><span><a href=\"#Missing-reports\" data-toc-modified-id=\"Missing-reports-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Missing reports</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mandatory-information\" data-toc-modified-id=\"Mandatory-information-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Mandatory information</a></span></li></ul></li><li><span><a href=\"#Incomplete-information-processing\" data-toc-modified-id=\"Incomplete-information-processing-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Incomplete information processing</a></span></li><li><span><a href=\"#About-date-normalization\" data-toc-modified-id=\"About-date-normalization-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>About date normalization</a></span></li></ul></li><li><span><a href=\"#Import-and-load\" data-toc-modified-id=\"Import-and-load-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import and load</a></span></li><li><span><a href=\"#Parse-XML-files\" data-toc-modified-id=\"Parse-XML-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Parse XML files</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sanity-check\" data-toc-modified-id=\"Sanity-check-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Sanity check</a></span></li></ul></li><li><span><a href=\"#Merge-files\" data-toc-modified-id=\"Merge-files-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Merge files</a></span></li><li><span><a href=\"#Make-DataFrame\" data-toc-modified-id=\"Make-DataFrame-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Make DataFrame</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remove-duplicate-case_id-and-report_id\" data-toc-modified-id=\"Remove-duplicate-case_id-and-report_id-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Remove duplicate case_id and report_id</a></span></li><li><span><a href=\"#Add-receiptdate\" data-toc-modified-id=\"Add-receiptdate-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Add receiptdate</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This script extract information from each xml files and save as dictionary in pickle format. The files are extracted in quater-wise. \n",
    "\n",
    "## Extracted information\n",
    "In this file, each report is extracted as one recording (assigned a dictionary key). The recording could contains multiple reactions or drugs. The data is formed as `dic = {key:value}` where key is the order/number of reports/recorddings, and the value is a list contains informative elements: [version, report_id, case_id, country, qualify, serious, serious_subtype_1, serious_subtype_2, serious_subtype_3, serious_subtype_4, serious_subtype_5, serious_subtype_6, receivedate, receiptdate, age, gender, weight, reaction_list, drug_list].\n",
    "\n",
    "<ol>\n",
    "    <li>Admin:</li> *version, report_id, case_id, country, qualify, serious, serious_subtype_1, serious_subtype_2, serious_subtype_3, serious_subtype_4, serious_subtype_5, serious_subtype_6, receivedate, receiptdate* are admin information. Index: 0-13               \n",
    "    <li>Demograph:</li> *age, gender, weight,* Index: 14-16\n",
    "    <li>Reaction:</li> reaction_list, (index:17); reaction_list contains arbitrary reactions, each reaction is a list with three elements (0-2): [PT_code, PT, outcome]\n",
    "    <li>Drug:</li> drug_list, (index:18); drug_list contains arbitrary drugs, each drug is a list with 16 elements (0-15): [char, product, dorse, unit, drugseparatedosagenumb, drugintervaldosageunitnumb,                             drugintervaldosagedefinition, form, route, indication, start_date, end_date, action, readm,                     additional, substance]  \n",
    "</ol>\n",
    "\n",
    "\n",
    "\n",
    "## Missing reports\n",
    "Each subtype (e.g., admin) contains <a href = \"#mandatory\">mandatory</a> and optional/voluntary information (defined by us). `miss_admin, miss_patient, miss_reaction, miss_drug` represents the number of reports that <i>Mandatory Information</i> missing administrative/patient/reaction/drug information, respectively. The reports with missed mandatory information are not included in our extracted file.\n",
    "\n",
    "### Mandatory information\n",
    "<ol>\n",
    "    <li>Admin:</li> country, qualify, serious                          \n",
    "    <li>Demograph:</li> age & gender (at least has one item)\n",
    "    <li>Reaction:</li> in each reaction, the PT (side effect) is mandatory. The reaction_list should contains at least 1 reaction; otherwise, this report will be ignored.\n",
    "    <li>Drug:</li> in each drug, substance are mandatory. The drug_list should contains at least 1 drug; otherwise, this report will be ignored.\n",
    "</ol>\n",
    "\n",
    "## Incomplete information processing\n",
    "\n",
    "<ul>\n",
    "    <li>Missing <code>qualify</code> is set as <code>'6'</code> while <code>'0'</code> refers to others (\\ie, there is a number but not in [1,6]). </li>\n",
    "    <li>Missing <code>receivedate</code>, <code>receiptdate</code> are set as <code>'0'</code>.</li>\n",
    "    <li>Missing <code>age</code>, <code>gender</code>,<code>weight</code> are set as <code>'0'</code>. If <code>age</code> and <code>gender</code> are both missing, ignore this report. gender= '0' indicates unknown sex. </li>\n",
    "    <li>Missing <code>PT_code</code>, <code>outcome</code> are set as <code>'0'</code> and <code>'6'</code> respectively.  For outcome, 6 refers to 'unknown'</li>\n",
    "    <li>Missing <code>char</code>, <code>dorse, unit, drugseparatedosagenumb,drugintervaldosageunitnumb, drugintervaldosagedefinition, form</code> are set as <code>'0'</code>.</li>\n",
    "    <li>Missing <code>product</code> is set as <code>none</code>.</li>\n",
    "    <li>Missing <code>start_date</code> is set as <code>'0'</code>; Missing <code>end_date</code> is set as <code>receiptdate</code>, or as <code>'0'</code> if <code>receiptdate</code> is also missing.</li>\n",
    "    <li>Missing <code>action</code> is set as <code>'5'</code>.</li>\n",
    "    <li>Missing <code>additional</code> is set as <code>'3'</code>.</li>\n",
    "    <li>Missing <code>readm</code> is set as <code>3</code> which means 'unknown'.</li>\n",
    "    <li>Missing <code>substance</code> is set as <code>'none'</code>.</li>\n",
    "</ul>\n",
    "The specific meanings of each code, please refer to `Code/Files/XML_NTS.pdf`. \n",
    "\n",
    "## About date normalization\n",
    "Since the dates (such as received date, receipt date, drug start/end date) are in different formats (e.g., yyyymmdd, yyyymm, yyyy), we normalize them as follows:\n",
    "<ul>\n",
    "    <li>Set the stand/calibrated date as <code>2000-1-1</code> (Day 0) and calculate the difference between current date and the stand date. For example, <code>2000-1-10</code> is normalized to <code>10</code> days.</li>\n",
    "    <li>All the dates without precise day are regarded as the 1st day of the month; likewise, the dates without specific month are regarded as January. For example, <code>2015</code> in yyyy formate is regarded as <code>2015-1-1</code>. Then it is normalized to <code>5479</code> days. </li>\n",
    "</ul>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load dic_reading.py\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "se_dic = pickle.load(open('../Data/curated/AE_mapping.pk', 'rb'))\n",
    "drug_dic = pickle.load(open('../Data/curated/drug_dic.pk', 'rb'))\n",
    "\n",
    "# In this MeDRA_dic, key is string of PT_name, value is a list:\n",
    "# [PT, PT_name, HLT,HLT_name,HLGT,HLGT_name,SOC,SOC_name,SOC_abbr]\n",
    "meddra_pd_all = pickle.load(open('../Data/curated/drug_mapping.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "def date_normalize(formate, dat): \n",
    "    stand_date = date(2000, 1, 1)\n",
    "    if formate=='102':  # the date is formed as yyyymmdd\n",
    "        current_date = date(int(dat[:4]), int(dat[4:6]), int(dat[6:8])) \n",
    "    elif formate=='610':  # formed as yyyymm\n",
    "        current_date = date(int(dat[:4]), int(dat[4:6]), 1)\n",
    "    elif formate=='602':  #formed as yyyy\n",
    "        current_date = date(int(dat[:4]), 1, 1)\n",
    "    delta = current_date - stand_date\n",
    "    return delta.days\n",
    "\n",
    "def days_to_date(days):\n",
    "    stand_date = date(2000, 1, 1)\n",
    "\n",
    "    if int(days)<0:\n",
    "        days = 1\n",
    "    \n",
    "    dt = datetime.fromordinal(int(days))\n",
    "    return dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am parsing: 2013q1\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2013q1/xml/ADR13Q1.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2013q1/xml/ADR13Q1.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/223016 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223016/223016 [00:24<00:00, 9155.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013q1 file saved. with 223016 reports\n",
      "I am parsing: 2013q2\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2013q2/xml/ADR13Q2.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2013q2/xml/ADR13Q2.xml\n",
      "171765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171764/171764 [00:14<00:00, 11484.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013q2 file saved. with 171764 reports\n",
      "I am parsing: 2013q3\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2013q3/xml/ADR13Q3.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2013q3/xml/ADR13Q3.xml\n",
      "185570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185569/185569 [00:16<00:00, 11442.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013q3 file saved. with 185569 reports\n",
      "I am parsing: 2013q4\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2013q4/xml/ADR13Q4.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2013q4/xml/ADR13Q4.xml\n",
      "232248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232247/232247 [00:21<00:00, 11028.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013q4 file saved. with 232247 reports\n",
      "I am parsing: 2014q1\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2014q1/xml/ADR14Q1.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2014q1/xml/ADR14Q1.xml\n",
      "260058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260057/260057 [00:24<00:00, 10539.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014q1 file saved. with 260057 reports\n",
      "I am parsing: 2014q2\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2014q2/xml/ADR14Q2.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2014q2/xml/ADR14Q2.xml\n",
      "223846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223845/223845 [00:19<00:00, 11416.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014q2 file saved. with 223845 reports\n",
      "I am parsing: 2014q3\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2014q3/xml/ADR14Q3.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2014q3/xml/ADR14Q3.xml\n",
      "211309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211308/211308 [00:19<00:00, 10950.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014q3 file saved. with 211308 reports\n",
      "I am parsing: 2014q4\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2014q4/xml/ADR14Q4.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2014q4/xml/ADR14Q4.xml\n",
      "207965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207964/207964 [00:19<00:00, 10918.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2014q4 file saved. with 207964 reports\n",
      "I am parsing: 2015q1\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2015q1/xml/ADR15Q1.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2015q1/xml/ADR15Q1.xml\n",
      "317072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 317071/317071 [00:43<00:00, 7269.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015q1 file saved. with 317071 reports\n",
      "I am parsing: 2015q2\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2015q2/xml/ADR15Q2.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2015q2/xml/ADR15Q2.xml\n",
      "289271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289270/289270 [00:29<00:00, 9765.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015q2 file saved. with 289270 reports\n",
      "I am parsing: 2015q3\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2015q3/xml/ADR15Q3.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2015q3/xml/ADR15Q3.xml\n",
      "398861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 398860/398860 [00:40<00:00, 9848.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015q3 file saved. with 398860 reports\n",
      "I am parsing: 2015q4\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2015q4/xml/ADR15Q4.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2015q4/xml/ADR15Q4.xml\n",
      "314705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314704/314704 [00:31<00:00, 9940.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015q4 file saved. with 314704 reports\n",
      "I am parsing: 2016q1\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2016q1/xml/ADR16Q1.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2016q1/xml/ADR16Q1.xml\n",
      "365683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 365682/365682 [00:36<00:00, 9912.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016q1 file saved. with 365682 reports\n",
      "I am parsing: 2016q2\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2016q2/xml/ADR16Q2.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2016q2/xml/ADR16Q2.xml\n",
      "316057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316056/316056 [00:38<00:00, 8290.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016q2 file saved. with 316056 reports\n",
      "I am parsing: 2016q3\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2016q3/xml/ADR16Q3.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2016q3/xml/ADR16Q3.xml\n",
      "313614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313613/313613 [00:38<00:00, 8105.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016q3 file saved. with 313613 reports\n",
      "I am parsing: 2016q4\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2016q4/xml/ADR16Q4.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2016q4/xml/ADR16Q4.xml\n",
      "309535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 309534/309534 [00:32<00:00, 9419.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016q4 file saved. with 309534 reports\n",
      "I am parsing: 2017q1\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2017q1/xml/ADR17Q1.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2017q1/xml/ADR17Q1.xml\n",
      "352914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 352913/352913 [00:35<00:00, 9883.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017q1 file saved. with 352913 reports\n",
      "I am parsing: 2017q2\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2017q2/xml/ADR17Q2.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2017q2/xml/ADR17Q2.xml\n",
      "337399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337398/337398 [00:36<00:00, 9204.13it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017q2 file saved. with 337398 reports\n",
      "I am parsing: 2017q3\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2017q3/xml/ADR17Q3.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2017q3/xml/ADR17Q3.xml\n",
      "351389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 351388/351388 [00:37<00:00, 9376.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017q3 file saved. with 351388 reports\n",
      "I am parsing: 2017q4\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2017q4/xml/ADR17Q4.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2017q4/xml/ADR17Q4.xml\n",
      "327849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 327848/327848 [00:33<00:00, 9677.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017q4 file saved. with 327848 reports\n",
      "I am parsing: 2018q1\n",
      "find 1 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q1/xml/ADR18Q1.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q1/xml/ADR18Q1.xml\n",
      "412703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412702/412702 [00:46<00:00, 8967.18it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018q1 file saved. with 412702 reports\n",
      "I am parsing: 2018q2\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q2/xml/1_ADR18Q2_format.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q2/xml/2_ADR18Q2_format.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q2/xml/3_ADR18Q2_format.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q2/xml/1_ADR18Q2_format.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q2/xml/2_ADR18Q2_format.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q2/xml/2_ADR18Q2_format.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q2/xml/3_ADR18Q2_format.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q2/xml/3_ADR18Q2_format.xml\n",
      "457172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 457169/457169 [00:45<00:00, 9999.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018q2 file saved. with 457169 reports\n",
      "I am parsing: 2018q3\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q3/xml/1_ADR18Q3.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q3/xml/2_ADR18Q3.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q3/xml/3_ADR18Q3.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q3/xml/1_ADR18Q3.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q3/xml/2_ADR18Q3.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q3/xml/2_ADR18Q3.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q3/xml/3_ADR18Q3.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q3/xml/3_ADR18Q3.xml\n",
      "420918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 420915/420915 [00:43<00:00, 9702.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018q3 file saved. with 420915 reports\n",
      "I am parsing: 2018q4\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q4/xml/1_ADR18Q4.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q4/xml/2_ADR18Q4.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q4/xml/3_ADR18Q4.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q4/xml/1_ADR18Q4.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q4/xml/2_ADR18Q4.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q4/xml/2_ADR18Q4.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q4/xml/3_ADR18Q4.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2018q4/xml/3_ADR18Q4.xml\n",
      "394069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 394066/394066 [00:38<00:00, 10249.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018q4 file saved. with 394066 reports\n",
      "I am parsing: 2019q1\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q1/xml/1_ADR19Q1.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q1/xml/2_ADR19Q1.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q1/xml/3_ADR19Q1.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q1/xml/1_ADR19Q1.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q1/xml/2_ADR19Q1.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q1/xml/2_ADR19Q1.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q1/xml/3_ADR19Q1.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q1/xml/3_ADR19Q1.xml\n",
      "413737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 413734/413734 [00:40<00:00, 10312.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019q1 file saved. with 413734 reports\n",
      "I am parsing: 2019q2\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q2/xml/1_ADR19Q2.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q2/xml/2_ADR19Q2.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q2/xml/3_ADR19Q2.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q2/xml/1_ADR19Q2.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q2/xml/2_ADR19Q2.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q2/xml/2_ADR19Q2.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q2/xml/3_ADR19Q2.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q2/xml/3_ADR19Q2.xml\n",
      "441111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441108/441108 [00:45<00:00, 9674.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019q2 file saved. with 441108 reports\n",
      "I am parsing: 2019q3\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q3/xml/1_ADR19Q3.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q3/xml/2_ADR19Q3.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q3/xml/3_ADR19Q3.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q3/xml/1_ADR19Q3.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q3/xml/2_ADR19Q3.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q3/xml/2_ADR19Q3.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q3/xml/3_ADR19Q3.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q3/xml/3_ADR19Q3.xml\n",
      "452876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 452873/452873 [00:46<00:00, 9733.93it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019q3 file saved. with 452873 reports\n",
      "I am parsing: 2019q4\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q4/xml/1_ADR19Q4.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q4/xml/2_ADR19Q4.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q4/xml/3_ADR19Q4.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q4/xml/1_ADR19Q4.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q4/xml/2_ADR19Q4.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q4/xml/2_ADR19Q4.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q4/xml/3_ADR19Q4.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2019q4/xml/3_ADR19Q4.xml\n",
      "419584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419581/419581 [01:27<00:00, 4817.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019q4 file saved. with 419581 reports\n",
      "I am parsing: 2020q1\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q1/XML/1_ADR20Q1.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q1/XML/2_ADR20Q1.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q1/XML/3_ADR20Q1.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q1/XML/1_ADR20Q1.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q1/XML/2_ADR20Q1.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q1/XML/2_ADR20Q1.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q1/XML/3_ADR20Q1.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q1/XML/3_ADR20Q1.xml\n",
      "460330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460327/460327 [00:46<00:00, 9854.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020q1 file saved. with 460327 reports\n",
      "I am parsing: 2020q2\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q2/XML/1_ADR20Q2.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q2/XML/2_ADR20Q2.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q2/XML/3_ADR20Q2.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q2/XML/1_ADR20Q2.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q2/XML/2_ADR20Q2.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q2/XML/2_ADR20Q2.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q2/XML/3_ADR20Q2.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q2/XML/3_ADR20Q2.xml\n",
      "429230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 429227/429227 [00:43<00:00, 9831.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020q2 file saved. with 429227 reports\n",
      "I am parsing: 2020q3\n",
      "find 3 files\n",
      "['/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q3/XML/1_ADR20Q3.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q3/XML/2_ADR20Q3.xml', '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q3/XML/3_ADR20Q3.xml']\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q3/XML/1_ADR20Q3.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q3/XML/2_ADR20Q3.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q3/XML/2_ADR20Q3.xml\n",
      "/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q3/XML/3_ADR20Q3.xml\n",
      "finished merge /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/2020q3/XML/3_ADR20Q3.xml\n",
      "431670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 431667/431667 [00:44<00:00, 9696.20it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020q3 file saved. with 431667 reports\n",
      "All data saved\n"
     ]
    }
   ],
   "source": [
    "# initial setup\n",
    "def date_normalize(formate, dat): \n",
    "    stand_date = date(2000, 1, 1)\n",
    "    if formate=='102':  # the date is formed as yyyymmdd\n",
    "        current_date = date(int(dat[:4]), int(dat[4:6]), int(dat[6:8])) \n",
    "    elif formate=='610':  # formed as yyyymm  \n",
    "        current_date = date(int(dat[:4]), int(dat[4:6]), 1)\n",
    "    elif formate=='602':  #formed as yyyy      \n",
    "        current_date = date(int(dat[:4]), 1, 1)\n",
    "    delta = current_date - stand_date\n",
    "    return delta.days\n",
    "\n",
    "n_reports = []\n",
    "miss_count = {}\n",
    "# To save time, parse 2018-2018  in the first round, then 2018-2021\n",
    "for yr in range(2013, 2021):\n",
    "\n",
    "    if yr == 2020:\n",
    "        qtr_list = [1, 2, 3]\n",
    "#         qtr_list = [3]\n",
    "    else:\n",
    "        qtr_list=[1,2,3,4]\n",
    "    for qtr in qtr_list:\n",
    "        qtr_name = str(yr)+'q'+ str(qtr)\n",
    "        print('I am parsing:',qtr_name)\n",
    "        \n",
    "#         \"\"\"Read data from lab storage\"\"\" \n",
    "#         /n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/\n",
    "        lab_storage = '/n/data1/hms/dbmi/zitnik/lab/datasets/2020-08-FAERS/data/'\n",
    "\n",
    "        files = lab_storage + qtr_name + '/**/**'\n",
    "        xml_files = glob.glob(files +\"/*.xml\", recursive=True)\n",
    "        unique_files = list(set(xml_files))  # only keep the unique values, remove duplicated files.\n",
    "        xml_files = unique_files\n",
    "        xml_files.sort()\n",
    "        print('find {} files'.format(len(xml_files)))\n",
    "        print(xml_files)\n",
    "\n",
    "        root = None\n",
    "        for xml_file in xml_files:\n",
    "            print(xml_file)\n",
    "            data = ET.parse(xml_file).getroot()\n",
    "            if root is None:\n",
    "                root = data\n",
    "            else:\n",
    "                root.extend(data)\n",
    "                print('finished merge',xml_file)\n",
    "        nmb_reports = len(root)\n",
    "        print(nmb_reports)\n",
    "\n",
    "        count = 0\n",
    "        patient_ID = 0\n",
    "        dic = {}\n",
    "        \n",
    "        miss_admin = miss_patient = miss_reaction = miss_drug =0\n",
    "        for report in tqdm(root.findall('safetyreport')):\n",
    "            \"\"\"Administrative Information\"\"\"\n",
    "#             report.find('').text\n",
    "            try:  # Mandatory Information: report_id\n",
    "                try:\n",
    "                    version = report.find('safetyreportversion').text\n",
    "                except:\n",
    "                    version = '1'\n",
    "                    \n",
    "                report_id = report.find('safetyreportid').text\n",
    "                \n",
    "                try:\n",
    "                    case_id = report.find('companynumb').text\n",
    "                except:\n",
    "                    case_id = '0'  # unknown case id\n",
    "                    \n",
    "                try:\n",
    "                    country = report.find('primarysource')[0].text\n",
    "                except:\n",
    "                    country = 'unknown'          \n",
    "\n",
    "                    \n",
    "                if country =='COUNTRY NOT SPECIFIED':\n",
    "                    country = 'unknown'\n",
    "                    \n",
    "                    \n",
    "                try:\n",
    "                    qualify = report.find('primarysource')[1].text\n",
    "                except:\n",
    "                    qualify = '6'  # the qualify is unknown\n",
    "                    \n",
    "#                 qualify = report.find('primarysource')[1].text\n",
    "                    \n",
    "                if qualify not in {'1', '2', '3', '4', '5', '6','7'}:\n",
    "                    qualify = '0'\n",
    "                                      \n",
    "                    \n",
    "                try:\n",
    "                    serious = report.find('serious').text\n",
    "                except:\n",
    "                    serious = '-1'\n",
    "                \n",
    "                try:\n",
    "                    s_1 = report.find('seriousnessdeath').text\n",
    "                except:\n",
    "                    s_1 = '0'\n",
    "                try:\n",
    "                    s_2 = report.find('seriousnesslifethreatening').text\n",
    "                except:\n",
    "                    s_2 = '0'\n",
    "                try:\n",
    "                    s_3 = report.find('seriousnesshospitalization').text\n",
    "                except:\n",
    "                    s_3 = '0'\n",
    "                try:\n",
    "                    s_4 = report.find('seriousnessdisabling').text\n",
    "                except:\n",
    "                    s_4 = '0'\n",
    "                try:\n",
    "                    s_5 = report.find('seriousnesscongenitalanomali').text\n",
    "                except:\n",
    "                    s_5 = '0'\n",
    "                try:\n",
    "                    s_6 = report.find('seriousnessother').text\n",
    "                except:\n",
    "                    s_6 = '0'\n",
    "                serious_subtype = [s_1, s_2, s_3, s_4, s_5, s_6]\n",
    "            except:\n",
    "                miss_admin +=1\n",
    "                continue\n",
    "\n",
    "            try:  # Optional information\n",
    "                # receivedate: Date when the report was the FIRST received\n",
    "                receivedateformat, receivedate = report.find('receivedateformat').text, report.find('receivedate').text\n",
    "                receivedate = date_normalize(receivedateformat, receivedate)\n",
    "            except:\n",
    "                receivedate = '0'\n",
    "            \n",
    "            try:\n",
    "                # receiptdate: Date of most RECENT report received\n",
    "                receiptdateformat, receiptdate = report.find('receiptdateformat').text, report.find('receiptdate').text\n",
    "                receiptdate = date_normalize(receiptdateformat, receiptdate)\n",
    "            except:\n",
    "                 receiptdate =  '0'\n",
    "\n",
    "            for patient in report.findall('patient'):\n",
    "                \"\"\"Demographic Information\"\"\"                \n",
    "                try:\n",
    "                    age = patient.find('patientonsetage').text\n",
    "                except:\n",
    "                    age = -1 # unknown age\n",
    "                try:\n",
    "                    ageunit = patient.find('patientonsetageunit').text\n",
    "                except:\n",
    "                    ageunit = '801' \n",
    "                # normalize age\n",
    "                try:\n",
    "                    age = int(age)  \n",
    "                    if age!= -1:\n",
    "                        if ageunit == '800':  # Decade \n",
    "                            age = '-1'\n",
    "                        elif ageunit == '801':  # Year\n",
    "                            age = age\n",
    "                        elif ageunit == '802':  # Month\n",
    "                            age = int(age/12)\n",
    "                        elif ageunit == '803':  # Week\n",
    "                            age = int(age/52)\n",
    "                        elif ageunit == '804':  # Day\n",
    "                            age = int(age/365)\n",
    "                        elif ageunit == '805':  # Hour\n",
    "                            age = int(age/(24*365))\n",
    "    #                     else:\n",
    "    #                         age = '-1'  # unknown age\n",
    "                except:\n",
    "                    age = -1\n",
    "                    \n",
    "                      \n",
    "                try:\n",
    "                    gender = patient.find('patientsex').text\n",
    "                except:\n",
    "                    gender = '0'\n",
    "                try:\n",
    "                    weight = patient.find('patientweight').text\n",
    "                except:\n",
    "                    weight = '0'\n",
    "                ## Nothing is mandatory\n",
    "#                 if age == -1 and gender== '0':  # Mandatory: if age & gender both missing, ignore this report.\n",
    "#                     miss_patient +=1\n",
    "#                     continue\n",
    "\n",
    "                reaction_list = []\n",
    "                for side_ in patient.findall('reaction'):\n",
    "                    try:  # outcome: 1-6, 6 levels in total\n",
    "                        try: \n",
    "                            PT_code = side_[0].text\n",
    "                        except:\n",
    "                            PT_code = '0'\n",
    "                        try:\n",
    "                            outcome = side_[2].text\n",
    "                        except:\n",
    "                            outcome = '6'\n",
    "                        try:\n",
    "                            PT = side_[1].text\n",
    "                        except:\n",
    "                            PT = 'none'\n",
    "                        reaction = [PT_code, PT, outcome]\n",
    "                    except:\n",
    "                        continue\n",
    "                    reaction_list.append(reaction) \n",
    "                if reaction_list.__len__() == 0:  # Mandatory condition: at least has one reaction\n",
    "                    miss_reaction += 1\n",
    "                    continue\n",
    "\n",
    "                drug_list = []\n",
    "                for drug_ in patient.findall('drug'):\n",
    "                    try:\n",
    "                        try:\n",
    "                            char =  drug_.find('drugcharacterization').text  # drugcharacterization: 1(suspect)/2(concomitant)/3(interacting)\n",
    "                        except:\n",
    "                            char = '0'\n",
    "                        try:\n",
    "                            product =  drug_.find('medicinalproduct').text  # drug brand\n",
    "                        except:\n",
    "                            product = 'none'\n",
    "                        \"\"\"Dosage are generally fixed according to the indication\"\"\"\n",
    "                        try: \n",
    "                            dorse, unit=  drug_.find('drugstructuredosagenumb').text, drug_.find('drugstructuredosageunit').text\n",
    "                            drugseparatedosagenumb, drugintervaldosageunitnumb, drugintervaldosagedefinition = \\\n",
    "                                drug_.find('drugseparatedosagenumb').text, drug_.find('drugintervaldosageunitnumb').text, \\\n",
    "                                drug_.find('drugintervaldosagedefinition').text\n",
    "                            form = drug_.find('drugdosageform').text  # tablet or capsule or sth \n",
    "                        except:\n",
    "                            dorse, unit, drugseparatedosagenumb,drugintervaldosageunitnumb, drugintervaldosagedefinition, form =\\\n",
    "                            '0', '0', '0','0','0', '0'\n",
    "                        try:\n",
    "                            route = drug_.find('drugadministrationroute').text\n",
    "                            if route == '048':\n",
    "                                route = '1'  # oral \n",
    "                            elif route == '061':\n",
    "                                route = '2'  # Topical\n",
    "                        except:\n",
    "                            route = '0'  # no information of route\n",
    "                        \n",
    "                        try:\n",
    "                            indication = drug_.find('drugindication').text  # indication (disease): super important\n",
    "                        except:\n",
    "                            indication = 'none'\n",
    "\n",
    "                        try:\n",
    "                            start_format, start_date = drug_.find('drugstartdateformat').text, drug_.find('drugstartdate').text\n",
    "                            start_date = date_normalize(start_format, start_date)\n",
    "                        except:\n",
    "                            start_date = '0'\n",
    "                        try:\n",
    "                            end_format, end_date = drug_.find('drugenddateformat').text, drug_.find('drugenddate').text\n",
    "                            end_date = date_normalize(end_format, end_date)\n",
    "                        except:\n",
    "                            try:\n",
    "                                end_date = receiptdate\n",
    "                            except:\n",
    "                                end_date = '0'\n",
    "                            \n",
    "                        try:\n",
    "                            action = drug_.find('actiondrug').text\n",
    "                        except:\n",
    "                            action = '5'\n",
    "                        try:\n",
    "                            additional = drug_.find('drugadditional').text\n",
    "                        except:\n",
    "                            additional = '3'\n",
    "                        try:\n",
    "                            readm = drug_.find('drugrecurreadministration').text\n",
    "                        except:\n",
    "                            readm = '3'\n",
    "                        try:\n",
    "                            substance = drug_.find('activesubstance')[0].text\n",
    "                        except:\n",
    "                            substance = 'none'\n",
    "                    except:  # Mandatory condition: if none of the above information is provided, ignore this report\n",
    "                        continue\n",
    "                    drug = [char, product, dorse, unit, drugseparatedosagenumb, drugintervaldosageunitnumb,\n",
    "                            drugintervaldosagedefinition, form, route, indication, start_date, end_date, action,\n",
    "                            readm, additional, substance]\n",
    "                    drug_list.append(drug)\n",
    "                if drug_list.__len__() ==0:\n",
    "                    miss_drug += 1\n",
    "                    continue\n",
    "\n",
    "                \"\"\"for patient_ID\"\"\"\n",
    "                dic[count] = [version, report_id, case_id, country, qualify, serious, \n",
    "                              s_1, s_2, s_3, s_4, s_5, s_6, \n",
    "                              receivedate, receiptdate,  \n",
    "                              age, gender, weight, reaction_list, drug_list]\n",
    "                count += 1\n",
    "\n",
    "        pickle.dump(dic, open('../Data/parsed/'+ qtr_name+'.pk', 'wb'))\n",
    "        \n",
    "        n_reports.append(len(dic))\n",
    "        print(qtr_name+' file saved. with', len(dic), 'reports')\n",
    "        miss_count[qtr_name] = [nmb_reports, miss_admin, miss_patient, miss_reaction, miss_drug]\n",
    "\n",
    "print ('All data saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419581"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of reports in 2019 Q4\n",
    "sep_2020 =pickle.load(open('../Data/parsed/2019q4.pk', 'rb'))\n",
    "len(sep_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>report_id</th>\n",
       "      <th>case_id</th>\n",
       "      <th>country</th>\n",
       "      <th>qualify</th>\n",
       "      <th>serious</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>receivedate</th>\n",
       "      <th>receiptdate</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>weight</th>\n",
       "      <th>SE</th>\n",
       "      <th>drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>10012352</td>\n",
       "      <td>US-PFIZER INC-2014070045</td>\n",
       "      <td>US</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5186</td>\n",
       "      <td>7243</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[22.1, Type 2 diabetes mellitus, 6], [22.1, A...</td>\n",
       "      <td>[[1, LIPITOR, 80, 003, 1, 1, 804, FILM-COATED ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10015533</td>\n",
       "      <td>US-PFIZER INC-2014070019</td>\n",
       "      <td>US</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5189</td>\n",
       "      <td>7243</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[22.1, Type 2 diabetes mellitus, 6], [22.1, S...</td>\n",
       "      <td>[[1, LIPITOR, 0, 0, 0, 0, 0, 0, 0, CARDIOVASCU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10018621</td>\n",
       "      <td>US-PFIZER INC-2014070521</td>\n",
       "      <td>US</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5190</td>\n",
       "      <td>7243</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[22.1, Type 2 diabetes mellitus, 6]]</td>\n",
       "      <td>[[1, LIPITOR, 0, 0, 0, 0, 0, 0, 1, LOW DENSITY...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  version report_id                   case_id country qualify serious s1 s2  \\\n",
       "0       2  10012352  US-PFIZER INC-2014070045      US       4       1  0  0   \n",
       "1       2  10015533  US-PFIZER INC-2014070019      US       4       1  0  0   \n",
       "2       2  10018621  US-PFIZER INC-2014070521      US       4       1  0  0   \n",
       "\n",
       "  s3 s4 s5 s6  receivedate  receiptdate age gender weight  \\\n",
       "0  0  0  0  1         5186         7243  -1      0      0   \n",
       "1  0  0  0  1         5189         7243  56      0      0   \n",
       "2  0  0  0  1         5190         7243  51      0      0   \n",
       "\n",
       "                                                  SE  \\\n",
       "0  [[22.1, Type 2 diabetes mellitus, 6], [22.1, A...   \n",
       "1  [[22.1, Type 2 diabetes mellitus, 6], [22.1, S...   \n",
       "2              [[22.1, Type 2 diabetes mellitus, 6]]   \n",
       "\n",
       "                                               drugs  \n",
       "0  [[1, LIPITOR, 80, 003, 1, 1, 804, FILM-COATED ...  \n",
       "1  [[1, LIPITOR, 0, 0, 0, 0, 0, 0, 0, CARDIOVASCU...  \n",
       "2  [[1, LIPITOR, 0, 0, 0, 0, 0, 0, 1, LOW DENSITY...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports_pd = pd.DataFrame(sep_2020.values(), \n",
    "                          columns=['version','report_id','case_id','country','qualify','serious',\n",
    "                                   's1','s2','s3','s4','s5','s6','receivedate','receiptdate',\n",
    "                                   'age','gender','weight','SE','drugs'])\n",
    "\n",
    "reports_pd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus on professional qualification, reports # 1 (93712, 19)\n",
      "Focus on professional qualification, reports # 2 (31645, 19)\n",
      "Focus on professional qualification, reports # 3 (97773, 19)\n",
      "Focus on professional qualification, reports # 4 (4796, 19)\n",
      "Focus on professional qualification, reports # 5 (170865, 19)\n",
      "Focus on professional qualification, reports # 6 (20282, 19)\n",
      "Focus on professional qualification, reports # 0 (508, 19)\n",
      "Focus on professional qualification, reports # 7 (0, 19)\n"
     ]
    }
   ],
   "source": [
    "for mm in ['1', '2', '3', '4', '5', '6','0', '7']:\n",
    "    id_qua22w = [h ==mm for h in reports_pd.qualify ]\n",
    "    reports_pd_sh = reports_pd[id_qua22w]  # professional: 1,2,3\n",
    "    print('Focus on professional qualification, reports #',mm, reports_pd_sh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am loading 2013q1 from ../Data/processed_2/v4/2013q1.pk\n",
      "loaded 223016\n",
      "(223016, 19)\n",
      "I am loading 2013q2 from ../Data/processed_2/v4/2013q2.pk\n",
      "loaded 171764\n",
      "(171764, 19)\n",
      "I am loading 2013q3 from ../Data/processed_2/v4/2013q3.pk\n",
      "loaded 185569\n",
      "(185569, 19)\n",
      "I am loading 2013q4 from ../Data/processed_2/v4/2013q4.pk\n",
      "loaded 232247\n",
      "(232247, 19)\n",
      "I am loading 2014q1 from ../Data/processed_2/v4/2014q1.pk\n",
      "loaded 260057\n",
      "(260057, 19)\n",
      "I am loading 2014q2 from ../Data/processed_2/v4/2014q2.pk\n",
      "loaded 223845\n",
      "(223845, 19)\n",
      "I am loading 2014q3 from ../Data/processed_2/v4/2014q3.pk\n",
      "loaded 211308\n",
      "(211308, 19)\n",
      "I am loading 2014q4 from ../Data/processed_2/v4/2014q4.pk\n",
      "loaded 207964\n",
      "(207964, 19)\n",
      "I am loading 2015q1 from ../Data/processed_2/v4/2015q1.pk\n",
      "loaded 317071\n",
      "(317071, 19)\n",
      "I am loading 2015q2 from ../Data/processed_2/v4/2015q2.pk\n",
      "loaded 289270\n",
      "(289270, 19)\n",
      "I am loading 2015q3 from ../Data/processed_2/v4/2015q3.pk\n",
      "loaded 398860\n",
      "(398860, 19)\n",
      "I am loading 2015q4 from ../Data/processed_2/v4/2015q4.pk\n",
      "loaded 314704\n",
      "(314704, 19)\n",
      "I am loading 2016q1 from ../Data/processed_2/v4/2016q1.pk\n",
      "loaded 365682\n",
      "(365682, 19)\n",
      "I am loading 2016q2 from ../Data/processed_2/v4/2016q2.pk\n",
      "loaded 316056\n",
      "(316056, 19)\n",
      "I am loading 2016q3 from ../Data/processed_2/v4/2016q3.pk\n",
      "loaded 313613\n",
      "(313613, 19)\n",
      "I am loading 2016q4 from ../Data/processed_2/v4/2016q4.pk\n",
      "loaded 309534\n",
      "(309534, 19)\n",
      "I am loading 2017q1 from ../Data/processed_2/v4/2017q1.pk\n",
      "loaded 352913\n",
      "(352913, 19)\n",
      "I am loading 2017q2 from ../Data/processed_2/v4/2017q2.pk\n",
      "loaded 337398\n",
      "(337398, 19)\n",
      "I am loading 2017q3 from ../Data/processed_2/v4/2017q3.pk\n",
      "loaded 351388\n",
      "(351388, 19)\n",
      "I am loading 2017q4 from ../Data/processed_2/v4/2017q4.pk\n",
      "loaded 327848\n",
      "(327848, 19)\n",
      "I am loading 2018q1 from ../Data/processed_2/v4/2018q1.pk\n",
      "loaded 412702\n",
      "(412702, 19)\n",
      "I am loading 2018q2 from ../Data/processed_2/v4/2018q2.pk\n",
      "loaded 457169\n",
      "(457169, 19)\n",
      "I am loading 2018q3 from ../Data/processed_2/v4/2018q3.pk\n",
      "loaded 420915\n",
      "(420915, 19)\n",
      "I am loading 2018q4 from ../Data/processed_2/v4/2018q4.pk\n",
      "loaded 394066\n",
      "(394066, 19)\n",
      "I am loading 2019q1 from ../Data/processed_2/v4/2019q1.pk\n",
      "loaded 413734\n",
      "(413734, 19)\n",
      "I am loading 2019q2 from ../Data/processed_2/v4/2019q2.pk\n",
      "loaded 441108\n",
      "(441108, 19)\n",
      "I am loading 2019q3 from ../Data/processed_2/v4/2019q3.pk\n",
      "loaded 452873\n",
      "(452873, 19)\n",
      "I am loading 2019q4 from ../Data/processed_2/v4/2019q4.pk\n",
      "loaded 419581\n",
      "(419581, 19)\n",
      "I am loading 2020q1 from ../Data/processed_2/v4/2020q1.pk\n",
      "loaded 460327\n",
      "(460327, 19)\n",
      "I am loading 2020q2 from ../Data/processed_2/v4/2020q2.pk\n",
      "loaded 429227\n",
      "(429227, 19)\n",
      "I am loading 2020q3 from ../Data/processed_2/v4/2020q3.pk\n",
      "loaded 431667\n",
      "(431667, 19)\n",
      "#- all reports 10443476\n",
      "The No. of high order combos: 10443476\n",
      "ho_combos saved ['18', '6161368', 'PHEH2006US13340', 'US', '5', '1', '1', '0', '1', '0', '0', '1', 2498, 4776, 68, '2', '0', ['10001488', '10002034', '10002855', '10002948', '10003210', '10003239', '10003246', '10003549', '10003598', '10003885', '10003988', '10005956', '10006451', '10006811', '10007649', '10007882', '10008118', '10008479', '10010214', '10010774', '10011078', '10011224', '10011906', '10011971', '10012267', '10012378', '10013654', '10013836', '10013950', '10013968', '10014062', '10014387', '10014971', '10016173', '10016256', '10017885', '10017955', '10018276', '10018282', '10018713', '10018884', '10019211', '10019465', '10019974', '10020028', '10020100', '10020772', '10021015', '10021024', '10021036', '10021519', '10022437', '10023232', '10025197', '10027141', '10028334', '10028411', '10029410', '10031009', '10031252', '10031282', '10033371', '10033425', '10033433', '10036772', '10037368', '10037596', '10038063', '10038435', '10039227', '10040831', '10041591', '10042128', '10042458', '10043882', '10044390', '10046571', '10047895', '10049438', '10050584', '10051728', '10051937', '10053573', '10058920', '10061246', '10061273', '10061619', '10062060', '10064658', '10067621', '10068319'], [], []]\n"
     ]
    }
   ],
   "source": [
    "ho_combos = {}\n",
    "\n",
    "nmb = 0 \n",
    "n_reports = []\n",
    "for yr in range(2013, 2021):\n",
    "\n",
    "    if yr == 2020:  \n",
    "        qtr_list = [1, 2, 3]\n",
    "    else:\n",
    "        qtr_list=[1,2,3,4]\n",
    "    for qtr in qtr_list:\n",
    "        qtr_name = str(yr)+'q'+ str(qtr)\n",
    "        file_path = '../Data/parsed/' +qtr_name +'.pk'\n",
    "        print('I am loading {} from {}'.format(qtr_name, file_path))\n",
    "        dic = pickle.load(open(file_path, 'rb'))\n",
    "        n_reports.append(len(dic))\n",
    "        print('loaded', len(dic))\n",
    "        values = np.array(list(dic.values()))\n",
    "        print(values.shape)   \n",
    "        \n",
    "        admin_demo = values[:, :17]\n",
    "        # Find all reactions/side effects in this quarter\n",
    "        se = values[:, 17] \n",
    "        drugs = values[:, 18]              \n",
    "        for i in range(values.shape[0]): # dive into a single report\n",
    "            new_se = []\n",
    "            new_drug = [] # empty mapped se and drugs\n",
    "            new_indication = [] \n",
    "            \n",
    "            se_report = se[i] \n",
    "            drugs_report = drugs[i]\n",
    "            for j in range(len(se_report)):  # dive into a single reaction\n",
    "                se_reaction = se_report[j][1].lower() \n",
    "                if '\\\\' in se_reaction:\n",
    "                    se_reaction = se_reaction.split('\\\\')[0] \n",
    "                if se_reaction in se_dic:\n",
    "                    se_key = se_reaction\n",
    "                elif ' ' in se_reaction:\n",
    "                    se_key_sets = se_reaction.split(' ')\n",
    "                    if se_key_sets[0] in se_dic:\n",
    "                        se_key = se_key_sets[0]\n",
    "                    elif se_key_sets[1] in se_dic:\n",
    "                        se_key = se_key_sets[1]                      \n",
    "                try: \n",
    "                    new_se.append(meddra_pd_all[se_reaction][0])   # Use MedDRA ID.\n",
    "                except:  # if the key not in se_dic, continue\n",
    "                    continue\n",
    "                \n",
    "            for k in range(len(drugs_report)):  # dive into a single drug\n",
    "                drug = drugs_report[k][-1].lower()   # the drug substance\n",
    "                indication = drugs_report[k][9].lower()  # the indication names\n",
    "                \n",
    "                \n",
    "                # find the proper drug_key that exist in drug_dic\n",
    "                if '\\\\' in drug:\n",
    "                    drug = drug.split('\\\\')[0] \n",
    "                if drug in drug_dic:\n",
    "                    drug_key = drug\n",
    "                elif ' ' in drug:\n",
    "                    key_sets = drug.split(' ')\n",
    "                    if key_sets[0] in drug_dic:\n",
    "                        drug_key = key_sets[0]\n",
    "\n",
    "                    elif key_sets[1] in drug_dic:\n",
    "                        drug_key = key_sets[1]               \n",
    "                try:\n",
    "#                     new_drug.append(drug_dic[drug_key][1])   # use code\n",
    "                    new_drug.append(drug_dic[drug_key][0])     # use drugbank ID\n",
    "                except:  # if the key not in drug_dic, continue\n",
    "                    continue\n",
    "                \n",
    "                # find the proper drug_key that exist in drug_dic\n",
    "                # Using meddra_pd_all\n",
    "                if '\\\\' in indication:\n",
    "                    indication = indication.split('\\\\')[0] \n",
    "                if indication in meddra_pd_all:\n",
    "                    indication_key = indication\n",
    "                elif ' ' in indication:\n",
    "                    key_sets = indication.split(' ')\n",
    "                    if key_sets[0] in meddra_pd_all:\n",
    "                        indication_key = key_sets[0]\n",
    "\n",
    "                    elif key_sets[1] in meddra_pd_all:\n",
    "                        indication_key = key_sets[1]               \n",
    "                try:\n",
    "                    new_indication.append(meddra_pd_all[indication_key][0])     # use MedDRA ID\n",
    "                except:  # if the key not in drug_dic, continue\n",
    "                    continue\n",
    "\n",
    "            # Feed into dictionary\n",
    "            new_se.sort()\n",
    "            new_drug.sort()  # keep the new_se and new_drug sorted, incase [1,2] and [2,1] appears as different drug list\n",
    "            new_indication.sort()\n",
    "\n",
    "            xx = list(admin_demo[i])\n",
    "            xx.append(new_se)\n",
    "            xx.append(new_drug)\n",
    "            xx.append(new_indication)\n",
    "            ho_combos[nmb] = xx   #[admin_demo[i], new_se, new_drug, new_indication]\n",
    "            nmb += 1\n",
    "            \n",
    "print('#- all reports', sum(n_reports))\n",
    "print('The No. of high order combos:',len(ho_combos))\n",
    "pickle.dump(ho_combos, open('../Data/parsed/reports_v4.pk', 'wb'))\n",
    "# Please revise 'Data/processed_2/v4/' to 'Data/parsed/'\n",
    "\n",
    "print('ho_combos saved', ho_combos.get(0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DataFrame\n",
    "\n",
    "## Remove duplicate case_id and report_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10443476"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports_v4 = pickle.load(open('../Data/parsed/reports_v4.pk', 'rb'))\n",
    "len(reports_v4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_pd = pd.DataFrame(reports_v4.values(), \n",
    "                          columns=['version','report_id','case_id','country','qualify','serious',\n",
    "                                   's1','s2','s3','s4','s5','s6','receivedate','receiptdate',\n",
    "                                   'age','gender','weight','SE','drugs','indications'])\n",
    "\n",
    "reports_pd['lastingdays'] =  reports_pd['receiptdate'] -reports_pd['receivedate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-of headache 352703\n",
      "#-SE: 19193, #-drugs: 3624 in #-reports:10443476\n"
     ]
    }
   ],
   "source": [
    "# check the #- of headache\n",
    "ix = ['10019211' in j for j in reports_pd.SE]\n",
    "print('#-of headache', sum(ix))\n",
    "\"\"\"# of drugs and AEs\"\"\"\n",
    "import itertools\n",
    "n_se = len(set(list(itertools.chain(*reports_pd.SE))))\n",
    "n_drug = len(set(list(itertools.chain(*reports_pd.drugs))))\n",
    "print('#-SE: {}, #-drugs: {} in #-reports:{}'.format(n_se, n_drug, len(reports_pd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>report_id</th>\n",
       "      <th>case_id</th>\n",
       "      <th>country</th>\n",
       "      <th>qualify</th>\n",
       "      <th>serious</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>...</th>\n",
       "      <th>receivedate</th>\n",
       "      <th>receiptdate</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>weight</th>\n",
       "      <th>SE</th>\n",
       "      <th>drugs</th>\n",
       "      <th>indications</th>\n",
       "      <th>lastingdays</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>6161368</td>\n",
       "      <td>PHEH2006US13340</td>\n",
       "      <td>US</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2498</td>\n",
       "      <td>4776</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[10001488, 10002034, 10002855, 10002948, 10003...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2278</td>\n",
       "      <td>2006-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7820167</td>\n",
       "      <td>US-ROCHE-735882</td>\n",
       "      <td>US</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4069</td>\n",
       "      <td>4764</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>[10000081, 10003246, 10009900, 10012378, 10013...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>695</td>\n",
       "      <td>2011-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>7683841</td>\n",
       "      <td>PHHY2009CA17701</td>\n",
       "      <td>CA</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3982</td>\n",
       "      <td>4777</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[10000081, 10000087, 10001949, 10003445, 10006...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>795</td>\n",
       "      <td>2010-11-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7684489</td>\n",
       "      <td>US-AMGEN-USASP2010000328</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3985</td>\n",
       "      <td>4777</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[10021519, 10023424, 10025197, 10028694, 10038...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>792</td>\n",
       "      <td>2010-11-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7823627</td>\n",
       "      <td>PHHO2011US03018</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4071</td>\n",
       "      <td>4783</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>[10064658]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>712</td>\n",
       "      <td>2011-02-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  version report_id                   case_id country qualify serious s1 s2  \\\n",
       "0      18   6161368           PHEH2006US13340      US       5       1  1  0   \n",
       "1       2   7820167           US-ROCHE-735882      US       4       1  0  0   \n",
       "2      11   7683841           PHHY2009CA17701      CA       3       1  0  0   \n",
       "3       2   7684489  US-AMGEN-USASP2010000328      US       3       1  0  0   \n",
       "4       2   7823627           PHHO2011US03018      US       3       1  0  0   \n",
       "\n",
       "  s3 s4  ... receivedate receiptdate  age  gender weight  \\\n",
       "0  1  0  ...        2498        4776   68       2      0   \n",
       "1  1  0  ...        4069        4764   39       2     68   \n",
       "2  1  0  ...        3982        4777   -1       2      0   \n",
       "3  1  0  ...        3985        4777   52       2      0   \n",
       "4  0  0  ...        4071        4783   57       2     56   \n",
       "\n",
       "                                                  SE drugs indications  \\\n",
       "0  [10001488, 10002034, 10002855, 10002948, 10003...    []          []   \n",
       "1  [10000081, 10003246, 10009900, 10012378, 10013...    []          []   \n",
       "2  [10000081, 10000087, 10001949, 10003445, 10006...    []          []   \n",
       "3  [10021519, 10023424, 10025197, 10028694, 10038...    []          []   \n",
       "4                                         [10064658]    []          []   \n",
       "\n",
       "  lastingdays        date  \n",
       "0        2278  2006-11-03  \n",
       "1         695  2011-02-21  \n",
       "2         795  2010-11-26  \n",
       "3         792  2010-11-29  \n",
       "4         712  2011-02-23  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command is time-consumming. \n",
    "reports_pd['date'] = reports_pd.apply(lambda row: str(pd.Period('2000-01-01')+int(row['receivedate'])), axis = 1)\n",
    "reports_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-of all reports (10443476, 22)\n",
      "After remove duplicate reports (9325731, 22)\n"
     ]
    }
   ],
   "source": [
    "print('#-of all reports',reports_pd.shape)\n",
    "reports_pd = reports_pd.drop_duplicates(['report_id', 'case_id','receivedate'], keep=\"last\")\n",
    "print('After remove duplicate reports',reports_pd.shape)\n",
    "\n",
    "# reports_pd = reports_pd.drop_duplicates(['case_id'], keep=\"last\")\n",
    "# print('After remove duplicate case_id',reports_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#-of headache 300301\n"
     ]
    }
   ],
   "source": [
    "# check the #- of headache\n",
    "ix = ['10019211' in j for j in reports_pd.SE]\n",
    "print('#-of headache', sum(ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reports_v4_pd_new saved (9325731, 22)\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(reports_pd, open('../Data/parsed/reports_v4_pd_new.pk', 'wb'))\n",
    "print('reports_v4_pd_new saved', reports_pd.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Add receiptdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reports_v4_pd_new saved (9325731, 22)\n"
     ]
    }
   ],
   "source": [
    "new_pd = pickle.load(open('../Data/parsed/reports_v4_pd_new.pk', 'rb'))\n",
    "print('reports_v4_pd_new saved', new_pd.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>report_id</th>\n",
       "      <th>case_id</th>\n",
       "      <th>country</th>\n",
       "      <th>qualify</th>\n",
       "      <th>serious</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>...</th>\n",
       "      <th>receiptdate</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>weight</th>\n",
       "      <th>SE</th>\n",
       "      <th>drugs</th>\n",
       "      <th>indications</th>\n",
       "      <th>lastingdays</th>\n",
       "      <th>date</th>\n",
       "      <th>receipt_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>6161368</td>\n",
       "      <td>PHEH2006US13340</td>\n",
       "      <td>US</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4776</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[10001488, 10002034, 10002855, 10002948, 10003...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>2278</td>\n",
       "      <td>2006-11-03</td>\n",
       "      <td>2013-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7820167</td>\n",
       "      <td>US-ROCHE-735882</td>\n",
       "      <td>US</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4764</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>[10000081, 10003246, 10009900, 10012378, 10013...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>695</td>\n",
       "      <td>2011-02-21</td>\n",
       "      <td>2013-01-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>7683841</td>\n",
       "      <td>PHHY2009CA17701</td>\n",
       "      <td>CA</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4777</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[10000081, 10000087, 10001949, 10003445, 10006...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>795</td>\n",
       "      <td>2010-11-26</td>\n",
       "      <td>2013-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7684489</td>\n",
       "      <td>US-AMGEN-USASP2010000328</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4777</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>[10021519, 10023424, 10025197, 10028694, 10038...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>792</td>\n",
       "      <td>2010-11-29</td>\n",
       "      <td>2013-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7823627</td>\n",
       "      <td>PHHO2011US03018</td>\n",
       "      <td>US</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4783</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>[10064658]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>712</td>\n",
       "      <td>2011-02-23</td>\n",
       "      <td>2013-02-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  version report_id                   case_id country qualify serious s1 s2  \\\n",
       "0      18   6161368           PHEH2006US13340      US       5       1  1  0   \n",
       "1       2   7820167           US-ROCHE-735882      US       4       1  0  0   \n",
       "2      11   7683841           PHHY2009CA17701      CA       3       1  0  0   \n",
       "3       2   7684489  US-AMGEN-USASP2010000328      US       3       1  0  0   \n",
       "4       2   7823627           PHHO2011US03018      US       3       1  0  0   \n",
       "\n",
       "  s3 s4  ... receiptdate age  gender  weight  \\\n",
       "0  1  0  ...        4776  68       2       0   \n",
       "1  1  0  ...        4764  39       2      68   \n",
       "2  1  0  ...        4777  -1       2       0   \n",
       "3  1  0  ...        4777  52       2       0   \n",
       "4  0  0  ...        4783  57       2      56   \n",
       "\n",
       "                                                  SE drugs indications  \\\n",
       "0  [10001488, 10002034, 10002855, 10002948, 10003...    []          []   \n",
       "1  [10000081, 10003246, 10009900, 10012378, 10013...    []          []   \n",
       "2  [10000081, 10000087, 10001949, 10003445, 10006...    []          []   \n",
       "3  [10021519, 10023424, 10025197, 10028694, 10038...    []          []   \n",
       "4                                         [10064658]    []          []   \n",
       "\n",
       "  lastingdays        date receipt_date  \n",
       "0        2278  2006-11-03   2013-01-28  \n",
       "1         695  2011-02-21   2013-01-16  \n",
       "2         795  2010-11-26   2013-01-29  \n",
       "3         792  2010-11-29   2013-01-29  \n",
       "4         712  2011-02-23   2013-02-04  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pd['receipt_date'] = new_pd.apply(lambda row: str(pd.Period('2000-01-01')+int(row['receiptdate'])), axis = 1)\n",
    "new_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_safety saved (9325731, 23)\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(new_pd, open('../Data/curated/patient_safety.pk', 'wb'))\n",
    "\n",
    "print('patient_safety saved', new_pd.shape) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
